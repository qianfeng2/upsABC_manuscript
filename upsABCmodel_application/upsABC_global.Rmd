---
title: "global_data_application"
author: "Qian Feng"
date: "24/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# This file is for applying upsABC classification method to an example dataset collected from global, so that we can see the assigned probabilities of ups types per DBLa sequence.


## Step 1. Global dataset preprocess. The purpose of this step is to translate all raw DNA DBLa sequences into proteins, as the input of our classification method is protein sequence. Also, we remove lab isolates in it and only focus on DBLa types. These two steps are divide into following two sub-steps.

**sub-step 1**: remove the non-translatable raw DBLa sequences and lab isolates.   
**sub-step 2**: generate OTU matrix from DNA sequences.  
 
Note that these two substeps with same data have been processed by Tonkin et al.2021, PloS Genetics. The script is in [supplementary file of Tonkin et al.2020](https://github.com/gtonkinhill/global_var_manuscript/blob/master/supplementary_methods_2.Rmd). 

Just note that for above sub-step2, the method for generating OTU matrix in Gerry's [cluster DBLa pipeline](https://github.com/UniMelb-Day-Lab/clusterDBLalpha/tree/master/third_party) adopts USEARCH software, authors provided two versions, one is *usearch8.1.1831*, one is *usearch8.1.1861_i86osx32*. When I reproduce results for these two steps, in order to run USEARCH successfully in my local computer, latter version is used.


Now we apply our classification method to this processed global dataset.


## Step 2. Construct Profile HMMs from Rask et al's data. For users with your own data, you can access the resulting .hmm directly from the directory: upsABC_manuscript/reference_database/ups* without rerunning the following script in this Step 2 and directly go to Step 3.


```{python eval=FALSE, include=FALSE}
## step1: First get all subclass's fasta files in each ups group
from mungo.fasta import FastaReader
import pandas as pd
Rask_data = pd.read_csv('/Users/fengqian/Downloads/upsABC_manuscript/reference_database/curated_data_set(Githinji_et_al).csv')
for index, row in Rask_data.iterrows():
    if row['ups']=="A":
        with open("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsA/"+row['domain']+".fasta", 'a+') as outfile:
            outfile.write(">"+row['Name']+"\n"+row['sequence']+"\n")
    if row['ups']=="B":
        with open("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsB/"+row['domain']+".fasta", 'a+') as outfile:
            outfile.write(">"+row['Name']+"\n"+row['sequence']+"\n")
    if row['ups']=="C":
        with open("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsC/"+row['domain']+".fasta", 'a+') as outfile:
            outfile.write(">"+row['Name']+"\n"+row['sequence']+"\n")

## step 2: align these files and then get hmm file.
import glob
import os
import sys
%cd /Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsA
for fastafile in glob.glob("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsA/*.fasta"):
    fastafile_name=fastafile.split("upsA/")[1]
    ID=[]
    for h,s in FastaReader(fastafile):ID.append(h)
    if len(ID)==1:
        cmd="hmmbuild "+str(fastafile_name.split(".fasta")[0])+".hmm "+str(fastafile_name.split(".fasta")[0])+".fasta"
        os.system(cmd)
    else:    
        cmd="clustalo -i "+str(fastafile_name)+" -o "+str(fastafile_name.split(".fasta")[0])+".sto --outfmt=st"
        os.system(cmd)        
        cmd="hmmbuild "+str(fastafile_name.split(".fasta")[0])+".hmm "+str(fastafile_name.split(".fasta")[0])+".sto"
        os.system(cmd)
%cd /Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsB
for fastafile in glob.glob("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsB/*.fasta"):
    fastafile_name=fastafile.split("upsB/")[1]
    ID=[]
    for h,s in FastaReader(fastafile):ID.append(h)
    if len(ID)==1:
        cmd="hmmbuild "+str(fastafile_name.split(".fasta")[0])+".hmm "+str(fastafile_name.split(".fasta")[0])+".fasta"
        os.system(cmd)
    else:
        cmd="clustalo -i "+str(fastafile_name)+" -o "+str(fastafile_name.split(".fasta")[0])+".sto --outfmt=st"
        os.system(cmd)        
        cmd="hmmbuild "+str(fastafile_name.split(".fasta")[0])+".hmm "+str(fastafile_name.split(".fasta")[0])+".sto"
        os.system(cmd)
%cd /Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsC
for fastafile in glob.glob("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/upsC/*.fasta"):
    fastafile_name=fastafile.split("upsC/")[1]
    ID=[]
    for h,s in FastaReader(fastafile):ID.append(h)
    if len(ID)==1:
        cmd="hmmbuild "+str(fastafile_name.split(".fasta")[0])+".hmm "+str(fastafile_name.split(".fasta")[0])+".fasta"
        os.system(cmd)
    else:
        cmd="clustalo -i "+str(fastafile_name)+" -o "+str(fastafile_name.split(".fasta")[0])+".sto --outfmt=st"
        os.system(cmd)        
        cmd="hmmbuild "+str(fastafile_name.split(".fasta")[0])+".hmm "+str(fastafile_name.split(".fasta")[0])+".sto"
        os.system(cmd)                
```



## Step 3, calculate P(x|Z,D).
I got 31929 DBLa sequences after Step 1. In this step, I firstly split global data into 106 splits, per split is composed of 300 sequences. This can help us to finish calculation in parallel on spartan when computing P(x|Z,D).

```{python eval=FALSE, include=FALSE}
# split global data into runs and remove sequences with "X"  
from mungo.fasta import FastaReader
import glob
import os, sys
from collections import defaultdict
seqs = {}
seq_list = []
count=0
with open("/Users/fengqian/Downloads/upsABC_manuscript/example_global_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta_mapping.txt", 'w') as outfile:
    for h,s in FastaReader("/Users/fengqian/Downloads/upsABC_manuscript/example_global_data/Protein_NoLab_translateable_combined_454_tessema_centroids.fasta"):
        if "X" in s: continue# 185 out of 31929 sequences contain special character "X"/ This character is not present in Rask et al's data, so we can not model it. Let's exclude these 185 sequences 
        count+=1
        outfile.write("seq" + str(count) + "\t" + h + "\n")
        seqs["seq" + str(count)] = s
        seq_list.append("seq" + str(count)) 
n=300;run=0
for i in range(0, len(seq_list), n):
    targets = seq_list[i:i+n]
    run+=1
    with open("/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/processed_data/split_files/Protein_NoLab_translateable_combined_454_tessema_centroids" + "_run" + str(run) + ".fasta", 'w') as outfile:
        for t in targets:
            outfile.write(">"+t+"\n"+seqs[t]+"\n")
```

Since 185 out of 31929 sequences contain special character "X". This character is not present in Rask et al's reference data, so we can not model it. We decided to exclude these 185 sequences. This leads to 31744 sequences in total. 


The following script is for calculating P(x|Z,D), which is implementated in spartan (high performance computer in UniMelb). A reminder is that the python script Model_applyto_global.py should be run in python 3 environment, also please make sure to install all related modules on spartan in advance. 

```{bash}
# Please run this in spartan. However you use other high performance computing instead of spartan, please follow your local instructions.
module load Python/3.6.4-intel-2017.u2
pip install pomegranate --user
```


```{bash eval=FALSE, include=FALSE}
#!/bin/bash
cd /data/gpfs/projects/punim0609/qian_feng/upsABC_manuscript/upsABCmodel_application/processed_data
for FILE in $(seq 1 1 106); do
echo sbatch -p mig --nodes=1 --job-name S1.${FILE} --account "punim0609" --ntasks=1 --cpus-per-task=1 --mem=20000 --mail-type=FAIL --mail-user=fengq2@student.unimelb.edu.au --time=0-2:0:00 -e "slurm-%A_%a.out" --wrap=\"python script/Model_applyto_global.py split_files/Protein_NoLab_translateable_combined_454_tessema_centroids_run${FILE}.fasta PAD_run${FILE}.csv PBD_run${FILE}.csv PCD_run${FILE}.csv\"

sbatch -p mig --nodes=1 --job-name S1.${FILE} --account "punim0609" --ntasks=1 --cpus-per-task=1 --mem=20000 --mail-type=FAIL --mail-user=fengq2@student.unimelb.edu.au --time=0-2:0:00 -e \"slurm-%A_%a.out\" --wrap="python script/Model_applyto_global.py split_files/Protein_NoLab_translateable_combined_454_tessema_centroids_run${FILE}.fasta PAD_run${FILE}.csv PBD_run${FILE}.csv PCD_run${FILE}.csv"

sleep 1

done


cd /data/gpfs/projects/punim0609/qian_feng/upsABC_manuscript/upsABCmodel_application/processed_data
find . -type f -empty -delete
```

After this step, you should obtain upsABCmodel_application/processed_data/PAD_run#.csv, PBD_run#.csv, PCD_run#.csv files. 


## Step 4, this is the last step! calculate final predictions of ups type per DBLa sequence, and write to csv.

```{r echo=FALSE}
library(data.table)
curated_data <- fread("/Users/fengqian/Downloads/upsABC_manuscript/reference_database/curated_data_set(Githinji_et_al).csv", header=TRUE, data.table = FALSE)
Rask_data <- curated_data[1:313,]
#dim(Rask_data)
visua_data=cbind(as.matrix(table(Rask_data$domain)),as.matrix(table(Rask_data$domain,Rask_data$ups)))
colnames(visua_data)=c("count","upsA","upsB","upsC")
prob_subclass <- visua_data[,1]/(colSums(visua_data)[1])
upsA_prob <- visua_data[,2]/visua_data[,1]
upsB_prob <- visua_data[,3]/visua_data[,1]
upsC_prob <- visua_data[,4]/visua_data[,1]
Rask_data_analyze <- as.data.frame(visua_data)
Rask_data_analyze$prob_subclass=prob_subclass
Rask_data_analyze$upsA_prob=upsA_prob
Rask_data_analyze$upsB_prob=upsB_prob
Rask_data_analyze$upsC_prob=upsC_prob
Rask_data_analyze
```



```{r echo=FALSE}
ListA <- list();ListB <- list();ListC <- list();
for (i in 1:106){
P_xAd=fread(paste("/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/processed_data/PAD_run",i,".csv",sep=""), skip = 1,data.table = TRUE)
P_xAd_matrix <- as.matrix(P_xAd[,2:ncol(P_xAd)]);rownames(P_xAd_matrix) <- P_xAd$V1
ListA[[i]] <- P_xAd_matrix

P_xBd=fread(paste("/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/processed_data/PBD_run",i,".csv",sep=""), skip = 1, data.table = TRUE)
P_xBd_matrix <- as.matrix(P_xBd[,2:ncol(P_xBd)]);rownames(P_xBd_matrix) <- P_xBd$V1
ListB[[i]] <- P_xBd_matrix

P_xCd=fread(paste("/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/processed_data/PCD_run",i,".csv",sep=""), skip = 1, data.table = TRUE)
P_xCd_matrix <- as.matrix(P_xCd[,2:ncol(P_xCd)]);rownames(P_xCd_matrix) <- P_xCd$V1
ListC[[i]] <- P_xCd_matrix

}
P_xAd_matrix = do.call(rbind, ListA)
P_xBd_matrix = do.call(rbind, ListB)
P_xCd_matrix = do.call(rbind, ListC)

soft_matrix <- matrix(0,31744,3);#31744 is my final total number of sequences, you need to change it for your own data
rownames(soft_matrix) <- rownames(P_xAd_matrix);colnames(soft_matrix) <- c("A","B","C")
hard_matrix <- matrix(0,2,3);
rownames(hard_matrix) <- c("Count","Normalized Prob");colnames(hard_matrix) <- c("A","B","C")
sum_soft_matrix <- matrix(0,2,3)
rownames(sum_soft_matrix) <- c("Sum Prob","Normalized Prob");colnames(sum_soft_matrix) <- c("A","B","C")
temp <- rep(0,31744)


for (i in 1:31744){
    #cat("sequence number:", i, "\n")
    P_xAd=rep(0,33);P_xBd=rep(0,33);P_xCd=rep(0,33);
    P_d=Rask_data_analyze$prob_subclass;
    P_Ad=Rask_data_analyze$upsA_prob;P_Bd=Rask_data_analyze$upsB_prob;P_Cd=Rask_data_analyze$upsC_prob
    
    P_xAd=exp(as.numeric(P_xAd_matrix[i,]));P_xBd=exp(as.numeric(P_xBd_matrix[i,]));P_xCd=exp(as.numeric(P_xCd_matrix[i,]));
    prob_A <- sum(P_xAd*P_d*P_Ad)
    prob_B <- sum(P_xBd*P_d*P_Bd)
    prob_C <- sum(P_xCd*P_d*P_Cd)
    soft_matrix[i,]=c(prob_A/(prob_A+prob_B+prob_C),prob_B/(prob_A+prob_B+prob_C),prob_C/(prob_A+prob_B+prob_C))
    max_index=which(soft_matrix[i,]==max(soft_matrix[i,]))
    if (length(max_index)>1){max_index <- sample(max_index,1)}
    temp[i]=max_index
    }


hard_matrix[1,]=c(length(which(temp==1)),length(which(temp==2)),length(which(temp==3)))
hard_matrix[2,]=hard_matrix[1,]/sum(hard_matrix[1,])
sum_soft_matrix[1,]=colSums(soft_matrix)
sum_soft_matrix[2,]=sum_soft_matrix[1,]/sum(sum_soft_matrix[1,])

#soft_matrix 
round(hard_matrix,3)
round(sum_soft_matrix,3)
write.csv(soft_matrix,"/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/result/result.csv")
```

From this, if we predict each sequence's ups type by using its largest ups type probability,(if largest type has two, random choose one) we finally identify 2120(6.7%) upsA, 22386(70.5%) upsB and 7238(22.8%) upsC type DBLa sequences.


## Analyze results. Following is a simple plot to visualize the upsABC classification for your results. Therefore, this is optional for user.
### See the distribution of three ups types.
```{r}
#pdf(file="/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/result/resuts_withoutjitter.pdf",width=14)
tbl=round(soft_matrix,3)
tbl=as.data.frame(tbl)
ggplot(tbl, aes(x=A, y=B)) +
  geom_point()+scale_y_continuous(breaks=seq(0.00,1.00,by=0.05),limits=c(0,1))+scale_x_continuous(breaks=seq(0.00,1.00,by=0.05),limits=c(0,1))+ylab("Predicted probability for upsB")+xlab("Predicted probability for upsA")+ theme_bw()+theme(axis.text.x = element_text(colour ="black",size=10),axis.text.y = element_text(colour ="black",size=10),axis.title.y = element_text(size = 12,margin = unit(c(0, 4, 0, 0), "mm")),axis.title.x = element_text(size = 12,margin = unit(c(4, 0, 0, 0), "mm")))+ theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_rect(colour = "black",size=1))
#dev.off()
```




From above plot, it is hard to see the results of upsB and upsC types, I decide to look at its density plot and jittered plot. This resulting plot indicate upsA is separated upsB&C, upsB and upsC also distinguish well. [I tried heatmap, its visualization is not as good as this!].

```{r}
#pdf(file="/Users/fengqian/Downloads/upsABC_manuscript/upsABCmodel_application/result/resuts_withjitter.pdf",width=14)
plot_main <- ggplot(tbl, aes(x=A, y=B)) +geom_point(size = 0.1, stroke = 0,shape=".")+scale_y_continuous(breaks=seq(0,1,by=0.1),limits=c(-0.02,1.02))+scale_x_continuous(breaks=seq(0,1,by=0.1),limits=c(-0.02,1.02))+ylab("Predicted probability for upsB")+xlab("Predicted probability for upsA")+theme_classic()+theme(axis.text.x = element_text(colour ="black",size=10),axis.text.y = element_text(colour ="black",size=10),axis.title.y = element_text(size = 12,margin = unit(c(0, 4, 0, 0), "mm")),axis.title.x = element_text(size = 12,margin = unit(c(4, 0, 0, 0), "mm")))+ geom_jitter(width = 0.01,height=0.01)#+ theme(panel.grid.major = element_blank(),panel.grid.minor = element_blank(),panel.border = element_rect(colour = "black",size=1))

plot_x <- axis_canvas(plot_main, axis = "x") +
  geom_density(aes(A), tbl)
plot_y <- axis_canvas(plot_main, axis = "y", coord_flip = TRUE) +
  geom_density(aes(B), tbl) +
  coord_flip()

# Combine all plots into one
plot_final <- insert_xaxis_grob(plot_main, plot_x, position = "top")
plot_final <- insert_yaxis_grob(plot_final, plot_y, position = "right")
ggdraw(plot_final)
#dev.off()
```




